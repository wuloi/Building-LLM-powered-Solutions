{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain's components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models and Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: What did the fish say when it hit the wall?\n",
      "A: Dam!\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "llm = OpenAI()\n",
    "print(llm('tell me a joke'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: the cat is on the table\n",
      "Translation in spanish:\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Sentence: {sentence}\n",
    "Translation in {language}:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "print(prompt.format(sentence = \"the cat is on the table\", language = \"spanish\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample CSV file \"sample.csv\" generated and saved.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    ['Name', 'Age', 'City'],\n",
    "    ['John', 25, 'New York'],\n",
    "    ['Emily', 28, 'Los Angeles'],\n",
    "    ['Michael', 22, 'Chicago']\n",
    "]\n",
    "\n",
    "# File name\n",
    "file_name = 'sample.csv'\n",
    "\n",
    "# Write data to CSV file\n",
    "with open(file_name, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(data)\n",
    "\n",
    "print(f'Sample CSV file \"{file_name}\" generated and saved.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Name: John\\nAge: 25\\nCity: New York', metadata={'source': 'sample.csv', 'row': 0}), Document(page_content='Name: Emily\\nAge: 28\\nCity: Los Angeles', metadata={'source': 'sample.csv', 'row': 1}), Document(page_content='Name: Michael\\nAge: 22\\nCity: Chicago', metadata={'source': 'sample.csv', 'row': 2})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='sample.csv')\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text file \"mountain.txt\" generated and saved.\n",
      "page_content=\"Amidst the serene landscape, towering mountains stand as majestic guardians of nature's beauty.\" metadata={}\n",
      "page_content='The crisp mountain air carries whispers of tranquility, while the rustling leaves compose a' metadata={}\n",
      "page_content='leaves compose a symphony of wilderness.' metadata={}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample sentences about mountains and nature\n",
    "content = \"\"\"Amidst the serene landscape, towering mountains stand as majestic guardians of nature's beauty.\n",
    "The crisp mountain air carries whispers of tranquility, while the rustling leaves compose a symphony of wilderness.\n",
    "Nature's palette paints the mountains with hues of green and brown, creating an awe-inspiring sight to behold.\n",
    "As the sun rises, it casts a golden glow on the mountain peaks, illuminating a world untouched and wild.\"\"\"\n",
    "\n",
    "# File name\n",
    "file_name = 'mountain.txt'\n",
    "\n",
    "# Write content to text file\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    txtfile.write(content)\n",
    "\n",
    "#print(f'Sample text file \"{file_name}\" generated and saved.')\n",
    "\n",
    "\n",
    "with open('mountain.txt') as f:\n",
    "    mountain = f.read()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([mountain])\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "print(texts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed documents:\n",
      "Number of vector: 5; Dimension of each vector: 1536\n",
      "Embed query:\n",
      "Dimension of the vector: 1536\n",
      "Sample of the first 5 elements of the vector: [0.00538721214979887, -0.0005941778072156012, 0.03892524912953377, -0.002979141427204013, -0.008912666700780392]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model ='text-embedding-ada-002' )\n",
    "\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Good morning!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"I want to report an accident\",\n",
    "        \"Sorry to hear that. May I ask your name?\",\n",
    "        \"Sure, Mario Rossi.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Embed documents:\")\n",
    "print(f\"Number of vector: {len(embeddings)}; Dimension of each vector: {len(embeddings[0])}\")\n",
    "\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "\n",
    "print(\"Embed query:\")\n",
    "print(f\"Dimension of the vector: {len(embedded_query)}\")\n",
    "print(f\"Sample of the first 5 elements of the vector: {embedded_query[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue text file \"dialogue.txt\" generated and saved.\n"
     ]
    }
   ],
   "source": [
    "#saving the conversation in a txt file\n",
    "# List of dialogue lines\n",
    "dialogue_lines = [\n",
    "    \"Good morning!\",\n",
    "    \"Oh, hello!\",\n",
    "    \"I want to report an accident\",\n",
    "    \"Sorry to hear that. May I ask your name?\",\n",
    "    \"Sure, Mario Rossi.\"\n",
    "]\n",
    "\n",
    "# File name\n",
    "file_name = 'dialogue.txt'\n",
    "\n",
    "# Write dialogue lines to text file\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    for line in dialogue_lines:\n",
    "        txtfile.write(line + '\\n')\n",
    "\n",
    "print(f'Dialogue text file \"{file_name}\" generated and saved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "\n",
    "raw_documents = TextLoader('dialogue.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0, separator = \"\\n\",)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to report an accident\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the reason for calling?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='I want to report an accident' metadata={'source': 'dialogue.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The reason for the call was to report an accident.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "query = \"What was the reason of the call?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '\\nThe human asked for ideas to write an essay in AI and the AI suggested writing on LLMs.'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))\n",
    "memory.save_context({\"input\": \"hi, I'm looking for some ideas to write an essay in AI\"}, {\"output\": \"hello, what about writing on LLMs?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mConversationSummaryMemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Dict[str, Any]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Dict[str, str]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'None'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m Save context from this conversation to buffer.\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\vaalt\\appdata\\local\\anaconda3\\lib\\site-packages\\langchain\\memory\\summary.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "ConversationSummaryMemory.save_context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' El gato está en la mesa.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "template = \"\"\"Sentence: {sentence}\n",
    "Translation in {language}:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "llm_chain.predict(sentence=\"the cat is on the table\", language=\"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "itinerary_template = \"\"\"You are a vacation itinerary assistant. \\\n",
    "You help customers finding the best destinations and itinerary. \\\n",
    "You help customer screating an optimized itinerary based on their preferences.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "restaurant_template = \"\"\"You are a restaurant booking assitant. \\\n",
    "You check with customers number of guests and food preferences. \\\n",
    "You pay attention whether there are special conditions to take into account.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"itinerary\",\n",
    "        \"description\": \"Good for creating itinerary\",\n",
    "        \"prompt_template\": itinerary_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"restaurant\",\n",
    "        \"description\": \"Good for help customers booking at restaurant\",\n",
    "        \"prompt_template\": restaurant_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "itinerary: {'input': \"I'm planning a trip from Milan to Venice by car. What attractions can I visit in between?\"}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Answer: \n",
      "There are many attractions that you can visit while traveling from Milan to Venice by car. Some of the most popular attractions include Lake Como, Verona, the Dolomites, and the picturesque towns of Bergamo and Brescia. You can also visit the stunning UNESCO World Heritage Sites in Mantua and Ferrara. Additionally, you can explore some of the local wineries and sample some of the wines of the region.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"I'm planning a trip from Milan to Venice by car. What can I visit in between?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "restaurant: {'input': 'I want to book a table for tonight'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      ". How many people are in your party?\n",
      "\n",
      "Hi there! How many people are in your party for tonight's reservation?\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"I want to book a table for tonight\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# This is an LLMChain to write a synopsis given a title of a play.\n",
    "llm = OpenAI(temperature=0)\n",
    "template = \"\"\"You are a comedian. Generate a joke on the following {topic}\n",
    "Joke:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
    "joke_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "template = \"\"\"You are translator. Given a text input, translate it to {language}\n",
    "Translation:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"language\"], template=template)\n",
    "translator_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Why did the cat cross the road? To prove to the dog that it could be done!\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m ¿Por qué cruzó el gato la carretera? ¡Para demostrarle al perro que se podía hacer!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[joke_chain, translator_chain], verbose=True)\n",
    "translated_joke = overall_chain.run(\"Cats and Dogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the string module\n",
    "import string\n",
    "\n",
    "# Define the function\n",
    "def rename_cat(inputs: dict) -> dict:\n",
    "  # Open the file in read mode\n",
    "  text = inputs[\"text\"]\n",
    "  # Create a table that maps punctuation characters to None\n",
    "  new_text = text.replace('cat', 'Silvester the Cat')\n",
    "  # Apply the table to the text and return the result\n",
    "  return {\"output_text\": new_text}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Silvester the Cat and a dog lived together but did not get along. Silvester the Cat played a prank on the dog which made him angry. When their owner found them fighting, she scolded them and made them apologize. After that, they became friends and learned to respect each other's differences and appreciate each other's strengths.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "with open(\"Cats&Dogs.txt\") as f:\n",
    "    cats_and_dogs = f.read()\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=rename_cat\n",
    ")\n",
    "\n",
    "template = \"\"\"Summarize this text:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\n",
    "llm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "\n",
    "sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])\n",
    "\n",
    "sequential_chain.run(cats_and_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"]\n",
    "\n",
    "search = SerpAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when Avatar 2 was released.\n",
      "Action: Search\n",
      "Action Input: \"Avatar 2 release date\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mDecember 16, 2022\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Avatar 2 was released on December 16, 2022.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Avatar 2 was released on December 16, 2022.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    )]\n",
    "\n",
    "agent = initialize_agent(tools, llm = OpenAI(), agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "agent.run(\"When was Avatar 2 released?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start working with LLMs in Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv   #installing the required package\n",
    "#!pip install huggingface_hub\n",
    "\n",
    "#option 1: get your tokens from the .env file\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 2: get the token with the getpass function\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain import HuggingFaceHub\n",
    "question = \"What was the first Disney movie?\"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: give a direct answer\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The first Disney movie was 'Snow White and the Seven Dwarfs', released in 193\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"tiiuae/falcon-7b-instruct\"  \n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 1000}\n",
    ")\n",
    "print(llm(\"what was the first disney movie?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
