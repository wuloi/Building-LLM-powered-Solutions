{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain的组件（components）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# import os\n",
    "# 接下来我们需要使用Google生成式AI 以及Google Search API，请自行申请，并将对应的 API Key 配置在.env中。\n",
    "# Google Generative AI - https://aistudio.google.com/app/apikey\n",
    "# os.environ[\"SERPAPI_API_KEY\"]\n",
    "# Google Search API - https://serpapi.com/\n",
    "# os.environ['GOOGLE_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型（Models）和 提示词（Prompts）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为什么树木很难相处？\n",
      "\n",
      "因为他们总是起冲突！ \n",
      " \n",
      "你喜欢那个吗？我可以再讲一个笑话。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "print(\n",
    "    llm.invoke(\n",
    "        \"说个笑话给我听\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子: 桌子上趴着一只猫\n",
      "翻译成 西班牙语:\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"句子: {sentence}\n",
    "翻译成 {language}:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "print(prompt.format(sentence = \"桌子上趴着一只猫\", language = \"西班牙语\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据连接（Data Connections）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成示例 CSV 文件“sample.csv” 并保存。\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    ['姓名', '年龄', '居住地'],\n",
    "    ['大姐姐', 25, 'New York'],\n",
    "    ['二妹子', 28, 'Los Angeles'],\n",
    "    ['三老板', 22, 'Chicago']\n",
    "]\n",
    "\n",
    "# File name\n",
    "file_name = 'sample.csv'\n",
    "\n",
    "# Write data to CSV file\n",
    "with open(file_name, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(data)\n",
    "\n",
    "print(f'已生成示例 CSV 文件“{file_name}” 并保存。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='姓名: 大姐姐\\n年龄: 25\\n居住地: New York', metadata={'source': 'sample.csv', 'row': 0}), Document(page_content='姓名: 二妹子\\n年龄: 28\\n居住地: Los Angeles', metadata={'source': 'sample.csv', 'row': 1}), Document(page_content='姓名: 三老板\\n年龄: 22\\n居住地: Chicago', metadata={'source': 'sample.csv', 'row': 2})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='sample.csv')\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='宁静的景色中，巍峨的山脉如同雄伟的守护者，守护着大自然的美丽。\\n清新的山间空气中弥漫着宁静的气息，沙沙作响的树叶谱写着荒野的交响乐。'\n",
      "page_content='大自然的调色板为山川披上了绿色和棕色的外衣，构成了一幅令人叹为观止的景象。\\n当太阳升起时，它为山峰披上了一层金色的光芒，照亮了一个未经触碰的狂野世界。'\n"
     ]
    }
   ],
   "source": [
    "# 关于山脉和自然的例句\n",
    "content = \"\"\"宁静的景色中，巍峨的山脉如同雄伟的守护者，守护着大自然的美丽。\n",
    "清新的山间空气中弥漫着宁静的气息，沙沙作响的树叶谱写着荒野的交响乐。\n",
    "大自然的调色板为山川披上了绿色和棕色的外衣，构成了一幅令人叹为观止的景象。\n",
    "当太阳升起时，它为山峰披上了一层金色的光芒，照亮了一个未经触碰的狂野世界。\"\"\"\n",
    "\n",
    "# 文件名\n",
    "file_name = 'mountain.txt'\n",
    "\n",
    "# 将内容写入文本文件\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    txtfile.write(content)\n",
    "\n",
    "#print(f'已生成示例文本文件“{file_name}”并保存。')\n",
    "\n",
    "\n",
    "with open('mountain.txt') as f:\n",
    "    mountain = f.read()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([mountain])\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入文档：\n",
      "向量数量：5；每个向量的维度：768\n",
      "嵌入查询：\n",
      "向量的维度：768\n",
      "向量的前 5 个元素示例：[0.04111874, -0.012125608, -0.060324237, -0.016665619, 0.06735335]\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "# 使用 Google 的 embedding-001 模型创建嵌入模型\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# 对一些句子进行嵌入\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"早上好！\",\n",
    "        \"哦，你好！\",\n",
    "        \"我想投诉一个商家\",\n",
    "        \"听到这个消息我很难过。请问您叫什么名字？\",\n",
    "        \"我叫孙悟空。\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 打印嵌入信息\n",
    "print(\"嵌入文档：\")\n",
    "print(f\"向量数量：{len(embeddings)}；每个向量的维度：{len(embeddings[0])}\")\n",
    "\n",
    "# 对查询语句进行嵌入\n",
    "embedded_query = embeddings_model.embed_query(\"对话中提到的名字是什么？\")\n",
    "\n",
    "# 打印查询嵌入信息\n",
    "print(\"嵌入查询：\")\n",
    "print(f\"向量的维度：{len(embedded_query)}\")\n",
    "print(f\"向量的前 5 个元素示例：{embedded_query[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成对话文本文件“dialogue.txt”并保存。\n"
     ]
    }
   ],
   "source": [
    "# 将对话保存到txt文件中\n",
    "# 对话行的列表\n",
    "dialogue_lines = [\n",
    "    \"早上好！\",\n",
    "    \"哦，你好！\",\n",
    "    \"我想投诉一个商家\",\n",
    "    \"听到这个消息我很难过。请问您叫什么名字？\",\n",
    "    \"我叫孙悟空。\"\n",
    "]\n",
    "\n",
    "# 文件名\n",
    "file_name = 'dialogue.txt'\n",
    "\n",
    "# 将对话行写入文本文件\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    for line in dialogue_lines:\n",
    "        txtfile.write(line + '\\n')\n",
    "\n",
    "print(f'已生成对话文本文件“{file_name}”并保存。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "# 加载文档，将其分割成块，嵌入每个块并将其加载到向量存储中。\n",
    "\n",
    "# 1. 加载文档\n",
    "raw_documents = TextLoader('dialogue.txt').load()  \n",
    "\n",
    "# 2. 分割文档\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0, separator = \"\\n\")\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# 3. 创建向量数据库并嵌入文档\n",
    "# 使用 Google 的 embedding-001 模型进行嵌入\n",
    "db = FAISS.from_documents(documents, GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "早上好！\n",
      "哦，你好！\n",
      "我想投诉一个商家\n",
      "听到这个消息我很难过。请问您叫什么名字？\n",
      "我叫孙悟空。\n"
     ]
    }
   ],
   "source": [
    "# 设置查询语句\n",
    "query = \"打电话的原因是什么？\" \n",
    "\n",
    "# 在向量数据库中进行相似性搜索\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# 打印最相似文档的内容\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='早上好！\\n哦，你好！\\n我想投诉一个商家\\n听到这个消息我很难过。请问您叫什么名字？\\n我叫孙悟空。' metadata={'source': 'dialogue.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '打电话的原因是什么？', 'result': '孙悟空打电话的原因是想投诉一个商家。 \\n'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "query = \"打电话的原因是什么？\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Current summary:\\n\\n\\nNew lines of conversation:\\nHuman: 你好，我正在寻找一些关于人工智能论文写作的主题\\nAI: 你好，写大型语言模型怎么样？\\n\\nNew summary:\\nThe human greets the AI and asks for topics to write about for a paper on artificial intelligence. The AI greets the human back and suggests writing about large language models. \\n'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\",temperature=0)\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# 创建 ConversationSummaryMemory 对象，用于存储和管理对话历史\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# 保存一段对话历史到内存中\n",
    "memory.save_context({\"input\": \"你好，我正在寻找一些关于人工智能论文写作的主题\"}, {\"output\": \"你好，写大型语言模型怎么样？\"})\n",
    "\n",
    "# 从内存中加载对话历史\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mConversationSummaryMemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Dict[str, Any]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Dict[str, str]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Save context from this conversation to buffer.\n",
      "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.11/site-packages/langchain/memory/summary.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "ConversationSummaryMemory.save_context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Spanish translation of the sentence \"桌上趴着一只猫\" depends on how you want to describe the cat\\'s position. Here are a few options:\\n\\n**Option 1: General position**\\n\\n* **Hay un gato en la mesa.** (There is a cat on the table.) - This is the most general translation and doesn\\'t specify the cat\\'s posture.\\n\\n**Option 2: Emphasizing lying down**\\n\\n* **Un gato está acostado sobre la mesa.** (A cat is lying on the table.) - This translation emphasizes that the cat is lying down.\\n\\n**Option 3: Specifying \"趴着\"**\\n\\n* **Un gato está echado sobre la mesa.** (A cat is sprawled/lying on the table.) - This translation uses \"echado\" which can imply a more relaxed or spread-out posture, closer to the meaning of \"趴着\".\\n\\nThe best option depends on the specific context and the nuance you want to convey. \\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"句子: {sentence}\n",
    "翻译成 {language}:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "llm_chain.predict(sentence=\"桌上趴着一只猫\", language=\"法语\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "# 定义行程规划助手的提示模板\n",
    "itinerary_template = \"\"\"你是一个旅行行程规划助手。\\\n",
    "你会帮助客户找到最佳的目的地和行程。\\\n",
    "你会根据客户的喜好帮助他们创建优化的行程。\n",
    "\n",
    "这是一个问题：\n",
    "{input}\"\"\"\n",
    "\n",
    "# 定义餐厅预订助手的提示模板\n",
    "restaurant_template = \"\"\"你是一个餐厅预订助手。\\\n",
    "你会与客户确认用餐人数和食物偏好。\\\n",
    "你会注意是否有需要考虑的特殊情况。\n",
    "\n",
    "这是一个问题：\n",
    "{input}\"\"\"\n",
    "\n",
    "# 存储提示信息\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"itinerary\",\n",
    "        \"description\": \"擅长创建行程\",\n",
    "        \"prompt_template\": itinerary_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"restaurant\",\n",
    "        \"description\": \"擅长帮助客户预订餐厅\",\n",
    "        \"prompt_template\": restaurant_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "# 为每个提示信息创建对应的 LLMChain\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "# 创建默认的对话链\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
    "\n",
    "# 创建路由器的提示信息\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "\n",
    "# 创建路由器\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "# 创建多提示链\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itinerary: {'input': '我计划从巴黎开车去里昂。我可以在中途参观什么地方？'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "太棒了！从巴黎开车到里昂是一次很棒的旅行，沿途有很多值得一看的地方。为了更好地帮助你规划行程，请告诉我你对什么样的中途停留感兴趣。 \n",
      "\n",
      "例如：\n",
      "\n",
      "* **你想走哪条路线？** 你更喜欢风景优美的路线，即使这意味着要开更长时间，还是想走最快的路线？\n",
      "* **你想在途中停留多久？** 是一天，还是更长时间？\n",
      "* **你对什么感兴趣？** 历史遗迹？风景优美的地方？品酒？其他什么？ \n",
      "\n",
      "一旦我了解了你的喜好，我就可以推荐一些很棒的中途停留地点，并帮助你规划完美的行程！ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"我计划从巴黎开车去里昂。我可以在中途参观什么地方？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant: {'input': '我想预订今晚的法国餐厅'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "太好了！我很乐意帮您预订法国餐厅。 \n",
      "\n",
      "首先，我想问您几个问题，以确保我能找到最适合您的餐厅。\n",
      "\n",
      "1. **您想预订几位？**\n",
      "2. **您有特别想去的法国餐厅吗？**\n",
      "3. **您希望几点用餐？**\n",
      "4. **您们有什么食物过敏或饮食要求吗？** \n",
      "5. **这是一个特殊的场合吗？例如生日或周年纪念日？**\n",
      "\n",
      "请提供以上信息，以便我为您找到最完美的餐厅！ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"我想预订今晚的法国餐厅\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型，并将 temperature 设置为 0\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 创建一个 LLMChain，用于根据给定的主题生成笑话\n",
    "template = \"\"\"你是一位喜剧演员。请根据以下主题 {topic} 生成一个笑话：\n",
    "笑话：\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
    "joke_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# 创建另一个 LLMChain，用于将文本翻译成指定语言\n",
    "template = \"\"\"你是一位翻译员。请将输入文本翻译成 {language}：\n",
    "翻译：\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"language\"], template=template)\n",
    "translator_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m请提供您需要翻译的文本，以及您希望翻译的目标语言。\n",
      "\n",
      "例如：\n",
      "\n",
      "**输入文本：** Bonjour, comment allez-vous ?\n",
      "**目标语言：** 中文\n",
      "\n",
      "我会将您的文本翻译成：“你好，你好吗？” \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 创建一个总链，依次运行笑话生成链和翻译链\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[joke_chain, translator_chain], verbose=True)\n",
    "translated_joke = overall_chain.run(\"猫猫和狗狗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入字符串模块\n",
    "import string\n",
    "\n",
    "# 定义函数\n",
    "def rename_cat(inputs: dict) -> dict:\n",
    "  # 获取输入文本\n",
    "  text = inputs[\"text\"]\n",
    "  # 将文本中的 \"cat\" 替换为 \"Silvester the Cat\"\n",
    "  new_text = text.replace('cat', 'Silvester the Cat')\n",
    "  # 返回替换后的文本\n",
    "  return {\"output_text\": new_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一只狡猾的猫和一只忠诚的狗住在一起，但相处得并不好，经常吵架。有一天，猫用毛线球捉弄狗，把毛线球绑在狗的尾巴上。狗很生气，追赶猫，他们打了起来。他们的主人被吵醒了，她很生气，训斥了他们，让他们和好。猫和狗都感到羞愧，向对方和主人道歉。从那天起，他们成了朋友，学会了尊重彼此，过上了幸福的生活。'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型，并将 temperature 设置为 0\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
    "\n",
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "with open(\"Cats&Dogs.txt\") as f:\n",
    "    cats_and_dogs = f.read()\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=rename_cat\n",
    ")\n",
    "\n",
    "template = \"\"\"总结这段文本:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "总结:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])\n",
    "\n",
    "sequential_chain.run(cats_and_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型，并将 temperature 设置为 0\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
    "\n",
    "from langchain import SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"]\n",
    "\n",
    "search = SerpAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: 寂静岭是什么时候上映的？\n",
      "Thought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"当你需要回答有关时事的问题时很有用\"\n",
    "    )]\n",
    "\n",
    "# 初始化agent，使用零样本反应描述代理\n",
    "agent = initialize_agent(tools, llm = llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# 运行agent，询问静岭的上映时间\n",
    "agent.run(\"寂静岭是什么时候上映的？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start working with LLMs in Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install python-dotenv   #installing the required package\n",
    "#!pip install huggingface_hub\n",
    "\n",
    "#option 1: get your tokens from the .env file\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 2: get the token with the getpass function\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "# 定义问题\n",
    "question = \"第一部法国电影是什么？？\"\n",
    "\n",
    "# 定义提示模板\n",
    "template = \"\"\"问题：{question}\n",
    "\n",
    "答案：给出直接的答案\"\"\"\n",
    "\n",
    "# 创建提示模板对象\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一部法国电影是什么？\n",
      "法国电影是什么\n",
      "法国电影是什么\n",
      "法国电影是什么\n",
      "法国电影是什么\n",
      "法国电影是什么\n",
      "法国电影是什么\n",
      "法国电影是什么\n",
      "法国电影\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"tiiuae/falcon-7b-instruct\"  \n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 1000}\n",
    ")\n",
    "print(llm(\"第一部法国电影是什么？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=PromptTemplate(input_variables=['question'], template='问题：{question}\\n\\n答案：给出直接的答案') llm=HuggingFaceHub(client=<InferenceClient(model='tiiuae/falcon-7b-instruct', timeout=None)>, repo_id='tiiuae/falcon-7b-instruct', task='text-generation', model_kwargs={'temperature': 0.5, 'max_length': 1000})\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(llm_chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
