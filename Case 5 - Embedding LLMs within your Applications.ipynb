{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain的组件（components）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# import os\n",
    "# 接下来我们需要使用Google生成式AI 以及Google Search API，请自行申请，并将对应的 API Key 配置在.env中。\n",
    "# Google Generative AI - https://aistudio.google.com/app/apikey\n",
    "# os.environ[\"SERPAPI_API_KEY\"]\n",
    "# Google Search API - https://serpapi.com/\n",
    "# os.environ['GOOGLE_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型（Models）和 提示词（Prompts）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为什么树木很难相处？\n",
      "\n",
      "因为他们总是起冲突！ \n",
      " \n",
      "你喜欢那个吗？我可以再讲一个笑话。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "print(\n",
    "    llm.invoke(\n",
    "        \"说个笑话给我听\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子: 桌子上趴着一只猫\n",
      "翻译成 西班牙语:\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"句子: {sentence}\n",
    "翻译成 {language}:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "print(prompt.format(sentence = \"桌子上趴着一只猫\", language = \"西班牙语\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据连接（Data Connections）\n",
    "\n",
    "定义: \n",
    "\n",
    "- 数据连接是指 LangChain 与各种不同数据源建立连接的方式，例如本地文件系统、云存储服务、数据库、网络 API 等。\n",
    "\n",
    "作用: \n",
    "\n",
    "- 通过数据连接，LangChain 可以方便地访问和读取各种数据，为后续的文本处理和分析提供数据基础。\n",
    "\n",
    "例子:\n",
    "\n",
    "- 读取本地文本文件\n",
    "- 连接到 AWS S3 存储桶读取文件\n",
    "- 查询 PostgreSQL 数据库获取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文档加载器 (Document Loaders):\n",
    "\n",
    "定义: \n",
    "\n",
    "- 文档加载器是 LangChain 中用于从不同数据源加载文档的组件。\n",
    "\n",
    "作用: \n",
    "- 文档加载器可以将各种格式的数据（如文本文件、PDF、HTML、Markdown 等）加载为 LangChain 能够处理的统一文档格式。\n",
    "\n",
    "例子:\n",
    "\n",
    "- TextLoader: 加载纯文本文件\n",
    "- PDFLoader: 加载 PDF 文件\n",
    "- UnstructuredFileLoader: 加载非结构化文本文件，如 Markdown、HTML 等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成示例 CSV 文件“sample.csv” 并保存。\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    ['姓名', '年龄', '居住地'],\n",
    "    ['大姐姐', 25, 'New York'],\n",
    "    ['二妹子', 28, 'Los Angeles'],\n",
    "    ['三老板', 22, 'Chicago']\n",
    "]\n",
    "\n",
    "# File name\n",
    "file_name = 'sample.csv'\n",
    "\n",
    "# Write data to CSV file\n",
    "with open(file_name, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(data)\n",
    "\n",
    "print(f'已生成示例 CSV 文件“{file_name}” 并保存。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='姓名: 大姐姐\\n年龄: 25\\n居住地: New York', metadata={'source': 'sample.csv', 'row': 0}), Document(page_content='姓名: 二妹子\\n年龄: 28\\n居住地: Los Angeles', metadata={'source': 'sample.csv', 'row': 1}), Document(page_content='姓名: 三老板\\n年龄: 22\\n居住地: Chicago', metadata={'source': 'sample.csv', 'row': 2})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='sample.csv')\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文档分割器 (Document Splitters):\n",
    "\n",
    "定义: \n",
    "\n",
    "- 文档分割器用于将大型文档分割成更小的文本块，方便后续处理。\n",
    "\n",
    "作用: \n",
    "\n",
    "- 由于语言模型通常有输入长度限制，将长文档分割成小块可以避免超出限制，同时也有利于提高处理效率和效果。\n",
    "\n",
    "例子:\n",
    "\n",
    "- CharacterTextSplitter: 按字符数分割文本\n",
    "- TokenTextSplitter: 按词数分割文本\n",
    "- MarkdownTextSplitter: 按 Markdown 标题分割文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='宁静的景色中，巍峨的山脉如同雄伟的守护者，守护着大自然的美丽。\\n清新的山间空气中弥漫着宁静的气息，沙沙作响的树叶谱写着荒野的交响乐。'\n",
      "page_content='大自然的调色板为山川披上了绿色和棕色的外衣，构成了一幅令人叹为观止的景象。\\n当太阳升起时，它为山峰披上了一层金色的光芒，照亮了一个未经触碰的狂野世界。'\n"
     ]
    }
   ],
   "source": [
    "# 关于山脉和自然的例句\n",
    "content = \"\"\"宁静的景色中，巍峨的山脉如同雄伟的守护者，守护着大自然的美丽。\n",
    "清新的山间空气中弥漫着宁静的气息，沙沙作响的树叶谱写着荒野的交响乐。\n",
    "大自然的调色板为山川披上了绿色和棕色的外衣，构成了一幅令人叹为观止的景象。\n",
    "当太阳升起时，它为山峰披上了一层金色的光芒，照亮了一个未经触碰的狂野世界。\"\"\"\n",
    "\n",
    "# 文件名\n",
    "file_name = 'mountain.txt'\n",
    "\n",
    "# 将内容写入文本文件\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    txtfile.write(content)\n",
    "\n",
    "#print(f'已生成示例文本文件“{file_name}”并保存。')\n",
    "\n",
    "\n",
    "with open('mountain.txt') as f:\n",
    "    mountain = f.read()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([mountain])\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本嵌入模型 (Text Embedding Models):\n",
    "\n",
    "定义: \n",
    "\n",
    "- 文本嵌入模型用于将文本转换为向量表示，捕捉文本的语义信息。\n",
    "\n",
    "作用: \n",
    "\n",
    "- 通过将文本转换为向量，可以计算文本之间的相似度，用于搜索、聚类等任务。\n",
    "\n",
    "例子:\n",
    "\n",
    "- OpenAI 的 text-embedding-ada-002 模型\n",
    "- Google 的 embedding-gecko 模型\n",
    "- SentenceTransformers 库提供的各种句子嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入文档：\n",
      "向量数量：5；每个向量的维度：768\n",
      "嵌入查询：\n",
      "向量的维度：768\n",
      "向量的前 5 个元素示例：[0.04111874, -0.012125608, -0.060324237, -0.016665619, 0.06735335]\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "# 使用 Google 的 embedding-001 模型创建嵌入模型\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# 对一些句子进行嵌入\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"早上好！\",\n",
    "        \"哦，你好！\",\n",
    "        \"我想投诉一个商家\",\n",
    "        \"听到这个消息我很难过。请问您叫什么名字？\",\n",
    "        \"我叫孙悟空。\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 打印嵌入信息\n",
    "print(\"嵌入文档：\")\n",
    "print(f\"向量数量：{len(embeddings)}；每个向量的维度：{len(embeddings[0])}\")\n",
    "\n",
    "# 对查询语句进行嵌入\n",
    "embedded_query = embeddings_model.embed_query(\"对话中提到的名字是什么？\")\n",
    "\n",
    "# 打印查询嵌入信息\n",
    "print(\"嵌入查询：\")\n",
    "print(f\"向量的维度：{len(embedded_query)}\")\n",
    "print(f\"向量的前 5 个元素示例：{embedded_query[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成对话文本文件“dialogue.txt”并保存。\n"
     ]
    }
   ],
   "source": [
    "# 将对话保存到txt文件中\n",
    "# 对话行的列表\n",
    "dialogue_lines = [\n",
    "    \"早上好！\",\n",
    "    \"哦，你好！\",\n",
    "    \"我想投诉一个商家\",\n",
    "    \"听到这个消息我很难过。请问您叫什么名字？\",\n",
    "    \"我叫孙悟空。\"\n",
    "]\n",
    "\n",
    "# 文件名\n",
    "file_name = 'dialogue.txt'\n",
    "\n",
    "# 将对话行写入文本文件\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    for line in dialogue_lines:\n",
    "        txtfile.write(line + '\\n')\n",
    "\n",
    "print(f'已生成对话文本文件“{file_name}”并保存。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量存储 (Vector Stores):\n",
    "\n",
    "定义: \n",
    "\n",
    "- 向量存储用于存储和检索文本嵌入向量。\n",
    "\n",
    "作用: \n",
    "\n",
    "- 向量存储可以高效地存储大量的文本向量，并支持快速相似性搜索，找到与查询文本语义相似的文本。\n",
    "\n",
    "例子:\n",
    "\n",
    "- Chroma\n",
    "- FAISS\n",
    "- Milvus\n",
    "- Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "# 加载文档，将其分割成块，嵌入每个块并将其加载到向量存储中。\n",
    "\n",
    "# 1. 加载文档\n",
    "raw_documents = TextLoader('dialogue.txt').load()  \n",
    "\n",
    "# 2. 分割文档\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0, separator = \"\\n\")\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# 3. 创建向量数据库并嵌入文档\n",
    "# 使用 Google 的 embedding-001 模型进行嵌入\n",
    "db = FAISS.from_documents(documents, GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "早上好！\n",
      "哦，你好！\n",
      "我想投诉一个商家\n",
      "听到这个消息我很难过。请问您叫什么名字？\n",
      "我叫孙悟空。\n"
     ]
    }
   ],
   "source": [
    "# 设置查询语句\n",
    "query = \"打电话的原因是什么？\" \n",
    "\n",
    "# 在向量数据库中进行相似性搜索\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# 打印最相似文档的内容\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='早上好！\\n哦，你好！\\n我想投诉一个商家\\n听到这个消息我很难过。请问您叫什么名字？\\n我叫孙悟空。' metadata={'source': 'dialogue.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检索器 (Retrievers):\n",
    "\n",
    "定义: \n",
    "\n",
    "- 检索器是 LangChain 中用于从向量存储中检索相关文档的组件。\n",
    "\n",
    "作用: \n",
    "\n",
    "- 检索器接收用户查询，将其转换为向量表示，然后在向量存储中搜索相似向量，返回对应的文档。\n",
    "\n",
    "例子:\n",
    "\n",
    "- VectorStoreRetriever: 从向量存储中检索文档\n",
    "- BM25Retriever: 使用 BM25 算法检索文档\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '打电话的原因是什么？', 'result': '孙悟空打电话的原因是想投诉一个商家。 \\n'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "query = \"打电话的原因是什么？\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Current summary:\\n\\n\\nNew lines of conversation:\\nHuman: 你好，我正在寻找一些关于人工智能论文写作的主题\\nAI: 你好，写大型语言模型怎么样？\\n\\nNew summary:\\nThe human greets the AI and asks for topics to write about for a paper on artificial intelligence. The AI greets the human back and suggests writing about large language models. \\n'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\",temperature=0)\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# 创建 ConversationSummaryMemory 对象，用于存储和管理对话历史\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# 保存一段对话历史到内存中\n",
    "memory.save_context({\"input\": \"你好，我正在寻找一些关于人工智能论文写作的主题\"}, {\"output\": \"你好，写大型语言模型怎么样？\"})\n",
    "\n",
    "# 从内存中加载对话历史\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mConversationSummaryMemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Dict[str, Any]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Dict[str, str]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Save context from this conversation to buffer.\n",
      "\u001b[0;31mFile:\u001b[0m      /usr/local/lib/python3.11/site-packages/langchain/memory/summary.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "ConversationSummaryMemory.save_context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains (链) \n",
    "\n",
    "Chains (链) 是 LangChain 中用于构建更复杂应用的核心概念，它允许将多个组件（如提示模板、语言模型、工具等）连接起来，形成一个处理流程。\n",
    "\n",
    "以下是您列出的几种常见链类型的解释：\n",
    "\n",
    "1. Simple Chain (简单链):\n",
    "\n",
    "- 定义: 最简单的链类型，由单个 LLM 或 Prompt 调用组成，通常用于执行单一任务。\n",
    "- 作用: 将输入传递给 LLM 或 Prompt，并将输出直接返回。\n",
    "- 例子: 使用简单链生成文本、翻译语言、总结文本等。\n",
    "\n",
    "2. Router Chain (路由链):\n",
    "\n",
    "- 定义: 根据输入或其他条件，将请求路由到不同的子链进行处理。\n",
    "- 作用: 根据不同的情况选择合适的处理路径，实现更灵活的逻辑。\n",
    "- 例子: 根据用户意图将请求路由到不同的对话机器人、根据问题类型选择不同的问答系统。\n",
    "\n",
    "3. Sequential Chain (顺序链):\n",
    "\n",
    "- 定义: 按顺序执行多个链，并将每个链的输出作为下一个链的输入。\n",
    "- 作用: 将多个步骤的任务连接起来，实现更复杂的功能。\n",
    "- 例子: 先对文本进行摘要，然后翻译成另一种语言，最后生成回复。\n",
    "\n",
    "4. Transformation Chain (转换链):\n",
    "\n",
    "- 定义: 对输入或输出进行转换，例如格式转换、数据增强等。\n",
    "- 作用: 预处理输入数据或后处理输出数据，以满足特定需求。\n",
    "- 例子: 将用户输入转换为规范格式、将模型输出转换为更易读的格式。\n",
    "\n",
    "LangChain 中的 Chains 提供了一种灵活且可扩展的方式来构建复杂的 LLM 应用。通过组合不同的链类型和组件，可以实现各种不同的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Chain (简单链):\n",
    "\n",
    "- 定义: 最简单的链类型，由单个 LLM 或 Prompt 调用组成，通常用于执行单一任务。\n",
    "- 作用: 将输入传递给 LLM 或 Prompt，并将输出直接返回。\n",
    "- 例子: 使用简单链生成文本、翻译语言、总结文本等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Spanish translation of the sentence \"桌上趴着一只猫\" depends on how you want to describe the cat\\'s position. Here are a few options:\\n\\n**Option 1: General position**\\n\\n* **Hay un gato en la mesa.** (There is a cat on the table.) - This is the most general translation and doesn\\'t specify the cat\\'s posture.\\n\\n**Option 2: Emphasizing lying down**\\n\\n* **Un gato está acostado sobre la mesa.** (A cat is lying on the table.) - This translation emphasizes that the cat is lying down.\\n\\n**Option 3: Specifying \"趴着\"**\\n\\n* **Un gato está echado sobre la mesa.** (A cat is sprawled/lying on the table.) - This translation uses \"echado\" which can imply a more relaxed or spread-out posture, closer to the meaning of \"趴着\".\\n\\nThe best option depends on the specific context and the nuance you want to convey. \\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"句子: {sentence}\n",
    "翻译成 {language}:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "llm_chain.predict(sentence=\"桌上趴着一只猫\", language=\"法语\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router Chain (路由链):\n",
    "\n",
    "- 定义: 根据输入或其他条件，将请求路由到不同的子链进行处理。\n",
    "- 作用: 根据不同的情况选择合适的处理路径，实现更灵活的逻辑。\n",
    "- 例子: 根据用户意图将请求路由到不同的对话机器人、根据问题类型选择不同的问答系统。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "# 定义行程规划助手的提示模板\n",
    "itinerary_template = \"\"\"你是一个旅行行程规划助手。\\\n",
    "你会帮助客户找到最佳的目的地和行程。\\\n",
    "你会根据客户的喜好帮助他们创建优化的行程。\n",
    "\n",
    "这是一个问题：\n",
    "{input}\"\"\"\n",
    "\n",
    "# 定义餐厅预订助手的提示模板\n",
    "restaurant_template = \"\"\"你是一个餐厅预订助手。\\\n",
    "你会与客户确认用餐人数和食物偏好。\\\n",
    "你会注意是否有需要考虑的特殊情况。\n",
    "\n",
    "这是一个问题：\n",
    "{input}\"\"\"\n",
    "\n",
    "# 存储提示信息\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"itinerary\",\n",
    "        \"description\": \"擅长创建行程\",\n",
    "        \"prompt_template\": itinerary_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"restaurant\",\n",
    "        \"description\": \"擅长帮助客户预订餐厅\",\n",
    "        \"prompt_template\": restaurant_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "# 为每个提示信息创建对应的 LLMChain\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "# 创建默认的对话链\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
    "\n",
    "# 创建路由器的提示信息\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "\n",
    "# 创建路由器\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "# 创建多提示链\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itinerary: {'input': '我计划从巴黎开车去里昂。我可以在中途参观什么地方？'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "太棒了！从巴黎开车到里昂是一次很棒的旅行，沿途有很多值得一看的地方。为了更好地帮助你规划行程，请告诉我你对什么样的中途停留感兴趣。 \n",
      "\n",
      "例如：\n",
      "\n",
      "* **你想走哪条路线？** 你更喜欢风景优美的路线，即使这意味着要开更长时间，还是想走最快的路线？\n",
      "* **你想在途中停留多久？** 是一天，还是更长时间？\n",
      "* **你对什么感兴趣？** 历史遗迹？风景优美的地方？品酒？其他什么？ \n",
      "\n",
      "一旦我了解了你的喜好，我就可以推荐一些很棒的中途停留地点，并帮助你规划完美的行程！ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"我计划从巴黎开车去里昂。我可以在中途参观什么地方？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant: {'input': '我想预订今晚的法国餐厅'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "太好了！我很乐意帮您预订法国餐厅。 \n",
      "\n",
      "首先，我想问您几个问题，以确保我能找到最适合您的餐厅。\n",
      "\n",
      "1. **您想预订几位？**\n",
      "2. **您有特别想去的法国餐厅吗？**\n",
      "3. **您希望几点用餐？**\n",
      "4. **您们有什么食物过敏或饮食要求吗？** \n",
      "5. **这是一个特殊的场合吗？例如生日或周年纪念日？**\n",
      "\n",
      "请提供以上信息，以便我为您找到最完美的餐厅！ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"我想预订今晚的法国餐厅\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain (顺序链):\n",
    "\n",
    "- 定义: 按顺序执行多个链，并将每个链的输出作为下一个链的输入。\n",
    "- 作用: 将多个步骤的任务连接起来，实现更复杂的功能。\n",
    "- 例子: 先对文本进行摘要，然后翻译成另一种语言，最后生成回复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型，并将 temperature 设置为 0\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 创建一个 LLMChain，用于根据给定的主题生成笑话\n",
    "template = \"\"\"你是一位喜剧演员。请根据以下主题 {topic} 生成一个笑话：\n",
    "笑话：\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
    "joke_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# 创建另一个 LLMChain，用于将文本翻译成指定语言\n",
    "template = \"\"\"你是一位翻译员。请将输入文本翻译成 {language}：\n",
    "翻译：\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"language\"], template=template)\n",
    "translator_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m请提供您需要翻译的文本，以及您希望翻译的目标语言。\n",
      "\n",
      "例如：\n",
      "\n",
      "**输入文本：** Bonjour, comment allez-vous ?\n",
      "**目标语言：** 中文\n",
      "\n",
      "我会将您的文本翻译成：“你好，你好吗？” \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 创建一个总链，依次运行笑话生成链和翻译链\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[joke_chain, translator_chain], verbose=True)\n",
    "translated_joke = overall_chain.run(\"猫猫和狗狗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Chain (转换链):\n",
    "\n",
    "- 定义: 对输入或输出进行转换，例如格式转换、数据增强等。\n",
    "- 作用: 预处理输入数据或后处理输出数据，以满足特定需求。\n",
    "- 例子: 将用户输入转换为规范格式、将模型输出转换为更易读的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入字符串模块\n",
    "import string\n",
    "\n",
    "# 定义函数\n",
    "def rename_cat(inputs: dict) -> dict:\n",
    "  # 获取输入文本\n",
    "  text = inputs[\"text\"]\n",
    "  # 将文本中的 \"cat\" 替换为 \"Silvester the Cat\"\n",
    "  new_text = text.replace('cat', 'Silvester the Cat')\n",
    "  # 返回替换后的文本\n",
    "  return {\"output_text\": new_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一只狡猾的猫和一只忠诚的狗住在一起，但相处得并不好，经常吵架。有一天，猫用毛线球捉弄狗，把毛线球绑在狗的尾巴上。狗很生气，追赶猫，他们打了起来。他们的主人被吵醒了，她很生气，训斥了他们，让他们和好。猫和狗都感到羞愧，向对方和主人道歉。从那天起，他们成了朋友，学会了尊重彼此，过上了幸福的生活。'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型，并将 temperature 设置为 0\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
    "\n",
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "with open(\"Cats&Dogs.txt\") as f:\n",
    "    cats_and_dogs = f.read()\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=rename_cat\n",
    ")\n",
    "\n",
    "template = \"\"\"总结这段文本:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "总结:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])\n",
    "\n",
    "sequential_chain.run(cats_and_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "**Agents (代理)** 是 LangChain 中更高级的概念，它能够根据目标自主地选择使用哪些工具和采取哪些行动。相比于 Chains 只能按照预定义的流程执行任务，Agents 更加灵活和智能。\n",
    "\n",
    "**Agent 的组成:**\n",
    "\n",
    "一个典型的 LangChain Agent 通常由以下几个部分组成:\n",
    "\n",
    "* **工具 (Tools):** Agent 可以使用的工具集，例如搜索引擎、数据库查询、代码执行器等。每个工具都有特定的功能和调用方式。\n",
    "* **语言模型 (Language Model):**  Agent 使用语言模型来理解用户的指令、制定计划以及生成调用工具的指令。\n",
    "* **代理策略 (Agent Policy):**  代理策略决定了 Agent 如何根据当前状态和目标选择合适的工具和行动。常见的代理策略包括：\n",
    "    * **零样本反应 (Zero-shot react):**  根据工具的描述选择合适的工具，不需要任何示例。\n",
    "    * **ReAct:**  一种迭代式的代理策略，Agent 会不断地观察环境、思考下一步行动，直到完成目标。\n",
    "* **内存 (Memory):**  Agent 可以使用内存来存储对话历史、中间结果等信息，帮助其做出更明智的决策。\n",
    "\n",
    "**Agent 的工作流程:**\n",
    "\n",
    "1. **接收用户指令:** Agent 接收用户的自然语言指令。\n",
    "2. **理解指令:** Agent 使用语言模型理解用户的意图和目标。\n",
    "3. **制定计划:** Agent 根据目标和可用的工具，制定一个行动计划。\n",
    "4. **执行计划:** Agent 按照计划调用相应的工具，并收集结果。\n",
    "5. **评估结果:** Agent 评估结果是否符合预期，如果不符合则调整计划并继续执行。\n",
    "6. **返回结果:**  Agent 将最终结果返回给用户。\n",
    "\n",
    "**Agent 的优势:**\n",
    "\n",
    "* **自主性:** Agent 可以根据目标自主地选择工具和行动，无需人工干预。\n",
    "* **灵活性:** Agent 可以处理各种不同的任务，并适应新的环境和工具。\n",
    "* **可扩展性:**  可以方便地为 Agent 添加新的工具和功能。\n",
    "\n",
    "**总结:**\n",
    "\n",
    "LangChain 中的 Agents 提供了一种构建更智能、更灵活的 LLM 应用的方式。通过组合不同的工具、语言模型和代理策略，可以创建能够解决各种复杂问题的 Agent。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "# 初始化语言模型，使用 Google 的 gemini-1.5-pro 模型，并将 temperature 设置为 0\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
    "\n",
    "from langchain import SerpAPIWrapper\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"SERPAPI_API_KEY\"]\n",
    "\n",
    "search = SerpAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: 寂静岭是什么时候上映的？\n",
      "Thought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: 我需要搜索寂静岭的上映日期。\n",
      "Action: Search\n",
      "Action Input: 寂静岭 上映日期\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mApril 21, 2006\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"当你需要回答有关时事的问题时很有用\"\n",
    "    )]\n",
    "\n",
    "# 初始化agent，使用零样本反应描述代理\n",
    "agent = initialize_agent(tools, llm = llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# 运行agent，询问静岭的上映时间\n",
    "agent.run(\"寂静岭是什么时候上映的？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start working with LLMs in Hugging Face Hub\n",
    "\n",
    "## 开始在 Hugging Face Hub 上使用大型语言模型 (LLMs)\n",
    "\n",
    "Hugging Face Hub 是一个强大的平台，提供了大量的预训练 LLM 以及便捷的工具，让您可以轻松地开始使用 LLMs 进行各种自然语言处理任务。以下是开始使用 Hugging Face Hub 上的 LLM 的步骤指南：\n",
    "\n",
    "**1. 安装必要的库**\n",
    "\n",
    "首先，确保您安装了必要的 Python 库。您可以使用 pip 安装 `transformers` 和 `langchain` 库：\n",
    "\n",
    "```bash\n",
    "pip install transformers langchain\n",
    "```\n",
    "\n",
    "**2. 选择预训练的 LLM**\n",
    "\n",
    "Hugging Face Hub 上提供了大量的预训练 LLM，您可以根据自己的需求选择合适的模型。以下是一些常用的 LLM：\n",
    "\n",
    "- **用于文本生成的模型:** GPT-2, GPT-Neo, Jurassic-1 Jumbo, BLOOM\n",
    "- **用于文本分类的模型:** BERT, RoBERTa, XLNet\n",
    "- **用于问答系统的模型:**  DistilBERT, ALBERT\n",
    "\n",
    "您可以访问 Hugging Face Model Hub ([https://huggingface.co/models](https://huggingface.co/models))，浏览可用的模型，并查看其描述、性能指标和使用方法。\n",
    "\n",
    "**3. 加载预训练的 LLM**\n",
    "\n",
    "选择好模型后，您可以使用 `transformers` 库轻松加载预训练的 LLM。以下是一个例子，演示如何加载 GPT-2 模型：\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 加载预训练的 GPT-2 模型\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "```\n",
    "\n",
    "**4. 使用 LangChain 调用 LLM**\n",
    "\n",
    "LangChain 提供了便捷的方式来调用 Hugging Face Hub 上的 LLM。以下是一个使用 LangChain 调用 GPT-2 模型生成文本的例子:\n",
    "\n",
    "```python\n",
    "from langchain import HuggingFaceHub, PromptTemplate, LLMChain\n",
    "\n",
    "# 设置 Hugging Face Hub API 密钥\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR_API_TOKEN\" \n",
    "\n",
    "# 初始化 Hugging Face Hub LLM\n",
    "llm = HuggingFaceHub(repo_id=\"gpt2\", model_kwargs={\"temperature\": 0.7, \"max_length\": 100})\n",
    "\n",
    "# 定义提示模板\n",
    "template = \"\"\"\n",
    "请根据以下内容续写故事：\n",
    "{context}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\"])\n",
    "\n",
    "# 创建 LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# 运行 LLMChain\n",
    "context = \"在一个遥远的星系中，\"\n",
    "output = llm_chain.run(context)\n",
    "\n",
    "# 打印输出\n",
    "print(output)\n",
    "```\n",
    "\n",
    "**5.  自定义提示模板**\n",
    "\n",
    "您可以根据自己的需求自定义提示模板，以引导 LLM 生成特定格式或内容的文本。例如，您可以指定生成文本的长度、主题、风格等。\n",
    "\n",
    "**6.  探索其他功能**\n",
    "\n",
    "除了文本生成，您还可以使用 Hugging Face Hub 上的 LLM 完成其他任务，例如：\n",
    "\n",
    "* **文本分类:** 对文本进行情感分析、主题分类等。\n",
    "* **问答系统:**  根据给定的文本回答问题。\n",
    "* **机器翻译:**  将文本翻译成其他语言。\n",
    "\n",
    "**总结:**\n",
    "\n",
    "Hugging Face Hub 为使用 LLM 提供了便捷的平台和工具。通过学习和使用 LangChain，您可以轻松地利用这些强大的模型来完成各种自然语言处理任务。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Hub 上使用大型语言模型 (LLMs) - DEMO:写科幻故事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install python-dotenv   #installing the required package\n",
    "#!pip install huggingface_hub\n",
    "\n",
    "#option 1: get your tokens from the .env file\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 2: get the token with the getpass function\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "# 定义问题\n",
    "context = \"在一个遥远的星系中，\"\n",
    "\n",
    "# 定义提示模板\n",
    "template = \"\"\"\n",
    "请根据以下内容续写故事：\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# 创建提示模板对象\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"tiiuae/falcon-7b-instruct\"  \n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 1000}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "请根据以下内容续写故事：\n",
      "在一个遥远的星系中，\n",
      "有一颗名为\"太阳\"的星球。\n",
      "太阳在星系中心，\n",
      "每天都在穿越\n",
      "星粒的光线。\n",
      "在每一个穿越的过程中，\n",
      "太阳都会穿越\n",
      "星粒的光线。\n",
      "太阳穿越星粒的光线，\n",
      "也穿越星粒的光\n"
     ]
    }
   ],
   "source": [
    "# 创建 LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# 运行 LLMChain\n",
    "context = \"在一个遥远的星系中，\"\n",
    "output = llm_chain.run(context)\n",
    "\n",
    "# 打印输出\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
